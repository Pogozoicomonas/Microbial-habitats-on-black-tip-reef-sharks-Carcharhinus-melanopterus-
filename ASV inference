######################################
#######ASV inference with DADA2#######
######################################

#adapted from https://benjjneb.github.io/dada2/tutorial.html

library(dada2)
library(ShortRead)
library(Biostrings)

setwd("~/Documents/Claudia.work/PostDoc/_Projects/Carcharhinus.melanopterus_microbial.habitats/sharks_ASVs_dada")
#generate matched lists of F and R read files and parse out sample names
fnFs <- sort(list.files("~/Documents/Claudia.work/sharks_ASVs_dada", pattern = "_R1_001.fastq", full.names = TRUE))
fnRs <- sort(list.files("~/Documents/Claudia.work/sharks_ASVs_dada", pattern = "_R2_001.fastq", full.names = TRUE)) 

#identify your primers.
FWD <- "AGGATTAGATACCCTGGTA"  ## my forward primer sequence, 5' --> 3'
REV <- "AGCRRCACGAGCTGACGAC"  ## my reverse primer sequence, 5' --> 3'

#verify presence and orientation of primer in seqs
allOrients <- function(primer) {  
  require(Biostrings)
  dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
  orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
               RevComp = reverseComplement(dna))
  return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients<-allOrients(FWD)
REV.orients<-allOrients(REV)
FWD.orients 
REV.orients

fnFs.filtN <- file.path("~/Documents/Claudia.work/sharks_ASVs_dada", "filtN", basename(fnFs))
fnRs.filtN <- file.path("~/Documents/Claudia.work/sharks_ASVs_dada", "filtN", basename(fnRs))
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)
 
length(fnFs)  
length(fnRs)  

#count number of reads in which the primer is found

primerHits <- function(primer, fn) {
   nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
  return(sum(nhits > 0))
}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[1]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[1]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))

#remove primers

cutadapt <- "/Applications/miniconda3/envs/cutadaptenv/bin/cutadapt"  
system2(cutadapt, args = "--version") #Version 2.9

path.cut <- file.path("~/Documents/Claudia.work/sharks_ASVs_dada", "cutadapt")
if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnRs.cut <- file.path(path.cut, basename(fnRs))

FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)

# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1.flags <- paste("-g", FWD, "-a", REV.RC) 

# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2.flags <- paste("-G", REV, "-A", FWD.RC) 

#Run Cutadapt
for(i in seq_along(fnFs)) {
  system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
                             "-o", fnFs.cut[i], "-p", fnRs.cut[i], # output files
                             fnFs.filtN[i], fnRs.filtN[i])) # input files
}

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[1]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[1]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]))

#Forward and reverse fastq filenames have the format:
cutFs <- sort(list.files(path.cut, pattern = "_R1_001.fastq", full.names = TRUE))
cutRs <- sort(list.files(path.cut, pattern = "_R2_001.fastq", full.names = TRUE))

# Extract sample names, assuming filenames have format:
get.sample.name <- function(fname) strsplit(basename(fname), "_")[[1]][1]
sample.names <- unname(sapply(cutFs, get.sample.name))
head(sample.names)

#visualize quality profiles of trimmed reads
plotQualityProfile(cutFs[1:2])
plotQualityProfile(cutRs[1:2])

#assigning output names for filtered reads
filtFs <- file.path(path.cut, "filtered", basename(cutFs))
filtRs <- file.path(path.cut, "filtered", basename(cutRs))
filtFs
filtRs

#filter reads
out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, maxN = 0, maxEE = c(2, 2), 
                     truncQ = 2, minLen = 50, rm.phix = TRUE, compress = TRUE, multithread = TRUE) #on windows, set multithread = FALSE
#Creating output directory: /Users/claudiapogoreutz/Documents/Claudia.work/sharks_ASVs_dada/cutadapt/filtered

#learn the error rates
errF <- learnErrors(filtFs, multithread = TRUE, nbases = 1e10)
errR <- learnErrors(filtRs, multithread = TRUE)

#plot error rates
plotErrors(errF,nominalQ = T)
plotErrors(errR,nominalQ = T) 

#dereplicate FASTQ files for faster processing
drepFs<-derepFastq(filtFs,verbose=T)  
drepRs<-derepFastq(filtRs,verbose=T) 

length(drepFs)  
length(drepRs)  

sample.names[keep]
sample.names 
names(drepFs)<-sample.names[keep]
names(drepRs)<-sample.names[keep]

#sample inference (core sample inference algorithm)
dadaFs <- dada(filtFs, err=errF, multithread=TRUE, pool = T)
dadaRs <- dada(filtRs, err=errR, multithread=TRUE, pool = T)

#merge read pairs
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
head(mergers[[1]]) 
length(mergers) 
 
#construct seq (ASV) table 
seqtab <- makeSequenceTable(mergers)
dim(seqtab)

# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab))) 

#remove chimeras
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)

#assess the proportion of chimeras in my ASV set?
sum(seqtab.nochim)/sum(seqtab)

#track numbers of reads through individual steps in your pipeline
getN <- function(x) sum(getUniques(x))
track <- cbind(out[,1][keep],out[,2][keep], sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
length(track[,1])

colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names[keep]
track

#annotation
#download silva_nr_v138_train_set.fa.gz" - vs. 138 train set
taxa <- assignTaxonomy(seqtab.nochim, "/Users/claudiapogoreutz/Documents/Claudia.work/sharks_ASVs_dada/silva_nr_v138_train_set.fa", multithread=TRUE) 

#inspect taxonomy assignments:
taxa.print <- taxa #Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)

#save tables
write.csv(taxa, "/Users/claudiapogoreutz/Documents/Claudia.work/sharks_ASVs_dada/shark_ASVs_taxa2.csv")
write.csv(seqtab.nochim, "/Users/claudiapogoreutz/Documents/Claudia.work/sharks_ASVs_dada/shark_ASVs_table2.csv")

